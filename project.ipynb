{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cele projektu:\n",
    "Rozpoznanie kierunku dźwięku nagrań binauralnych pojedynczych próbek białego szumu. Rozpoznawanie kierunku poziomego lub pionowego. Porównanie metryk sukcesu klasyfikatora dla próbek pochodzących z nagrań binauralnych i sygnałów wygenerowanych przy pomocy hrtf'ów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilioteki\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sofa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import sys,glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from spafe.features.gfcc import gfcc\n",
    "import pdb\n",
    "import functools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Efficient_ccf(x1,x2):\n",
    "    \"\"\"calculate cross-crrelation function in frequency domain, which is more \n",
    "    efficient than the direct calculation\"\"\"\n",
    "    \n",
    "    if x1.shape[0] != x2.shape[0]:\n",
    "        raise Exception('length mismatch')\n",
    "    wav_len = x1.shape[0]\n",
    "    # hanning window before fft\n",
    "    wf = np.hanning(wav_len)\n",
    "    x1 = x1*wf\n",
    "    x2 = x2*wf\n",
    "\n",
    "    X1 = np.fft.fft(x1,2*wav_len-1)# equivalent to add zeros \n",
    "    X2 = np.fft.fft(x2,2*wav_len-1)\n",
    "    ccf_unshift = np.real(np.fft.ifft(np.multiply(X1,np.conjugate(X2))))\n",
    "    ccf = np.concatenate([ccf_unshift[wav_len:],ccf_unshift[:wav_len]],axis=0)\n",
    "    \n",
    "    return ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ITD(x,fs,max_delay=None,inter_method='parabolic'):\n",
    "    \"\"\"\n",
    "    estimate ITD based on interaural corss-correlation function\n",
    "    itd = chann0_delay - chann1_delay\n",
    "    corr(i) = sum(x0[t]*x1[t-i])\n",
    "        | >0 chann1 lead\n",
    "    itd |\n",
    "        | <0 chann1 lead\n",
    "    input: \n",
    "        max_delay: maximum value of ITD, default value: 1ms\n",
    "        inter_method: method of ccf interpolation, \"None\"(default),\"parabolic\",'exponential'.\n",
    "    \"\"\"\n",
    "    wav_len = x.shape[0]\n",
    "    \n",
    "    # detrend\n",
    "    # x_detrend = x-np.mean(x,axis=0)\n",
    "    x_detrend = x\n",
    "    \n",
    "    if max_delay == None:\n",
    "        max_delay = int(1e-3*fs)\n",
    "    \n",
    "    if False:\n",
    "        # time domain\n",
    "        ccf_full = np.correlate(x_detrend[:,0],x_detrend[:,1],mode='full')\n",
    "        ccf = ccf_full[wav_len-1-max_delay:wav_len+max_delay]\n",
    "    else:\n",
    "        # frequency domain\n",
    "        ccf_full = Efficient_ccf(x_detrend[:,0],x_detrend[:,1])\n",
    "        ccf = ccf_full[wav_len-1-max_delay:wav_len+max_delay]\n",
    "    \n",
    "    ccf_std = ccf/(np.sqrt(np.sum(x_detrend[:,0]**2)*np.sum(x_detrend[:,1]**2)))\n",
    "    max_pos = np.argmax(ccf)\n",
    "    \n",
    "    ######################\n",
    "    if False:\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.subplot(311);    plt.plot(ccf_std);\n",
    "        plt.plot([wav_len-max_delay-1,wav_len-max_delay-1],[0,1],'r')\n",
    "        plt.plot([wav_len+max_delay-1,wav_len+max_delay-1],[0,1],'r')\n",
    "        plt.plot(wav_len-1-max_delay+max_pos,ccf_std[wav_len-1-max_delay+max_pos],'x',linewidth=2)\n",
    "\n",
    "        plt.subplot(312);    plt.plot(x[:,0])\n",
    "        plt.subplot(313);    plt.plot(x[:,1])\n",
    "\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.001)\n",
    "    ######################\n",
    "    \n",
    "    # exponential interpolation \n",
    "    delta = 0\n",
    "    if inter_method == 'exponential':\n",
    "        if max_pos> 0 and max_pos < max_delay*2-2:\n",
    "            if np.min(ccf[max_pos-1:max_pos+2]) > 0:\n",
    "                delta = (np.log(ccf[max_pos+1])-np.log(ccf[max_pos-1]))/\\\n",
    "                            (4*np.log(ccf[max_pos])-\n",
    "                             2*np.log(ccf[max_pos-1])-\n",
    "                             2*np.log(ccf[max_pos+1]))\n",
    "    elif inter_method == 'parabolic':\n",
    "        if max_pos> 0 and max_pos < max_delay*2-2:\n",
    "            delta = (ccf[max_pos-1]-ccf[max_pos+1])/(2*(ccf[max_pos+1]-2*ccf[max_pos]+ccf[max_pos-1]))\n",
    "        \n",
    "    ITD = float((max_pos-max_delay-1+delta))/fs*1e3\n",
    "\n",
    "    return [ITD,ccf_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(array, sr):\n",
    "    \n",
    "    params = []\n",
    "    for i in range(2):\n",
    "        params.append(np.mean(array[i]))  #statystyka\n",
    "        params.append(np.std(array[i]))\n",
    "        params.append(np.median(array[i]))\n",
    "        params.append(np.percentile(array[i], 25))\n",
    "        params.append(np.percentile(array[i], 75))\n",
    "        params.append(scipy.stats.iqr(array[i], rng=(10, 90)))\n",
    "        params.append(scipy.stats.kurtosis(array[i]))\n",
    "        params.append(scipy.stats.skew(array[i]))\n",
    "        params.append(np.min(array[i]))\n",
    "        params.append(np.max(array[i]))\n",
    "        \n",
    "        params.append(np.mean(librosa.feature.spectral_centroid(array[i], sr=sr)))#cechy spectrum(spectral centroiod, spectral rollof)\n",
    "        params.append(np.mean(librosa.feature.spectral_rolloff(array[i], sr=sr)))\n",
    "        \n",
    "        #params.extend(librosa.feature.mfcc(array[i], sr=sr, n_mfcc=13).flatten())#MFCC i GFCC\n",
    "        #params.extend(gfcc(array[i], fs=sr, num_ceps=13, nfft=sr).flatten())\n",
    "        \n",
    "        #params.append(librosa.feature.zero_crossing_rate(array[i], frame_length=sr))#RMS ratio, zero crossing rate\n",
    "        #params.append(librosa.feature.rms(array[i], frame_length=sr))\n",
    "        \n",
    "        \n",
    "    length = len(params)\n",
    "    idx = length//2\n",
    "    l = params[:idx]\n",
    "    r = params[idx:]\n",
    "    for i in range(len(l)):\n",
    "        params.append(np.mean([l[i],r[i]]))\n",
    "        params.append(np.std([l[i],r[i]]))\n",
    "    params.append(10*np.log10(np.sum(array[:,1]**2)/np.sum(array[:,0]**2)+1e-10)) #ILD, ILC\n",
    "    params.append(get_ITD(array, sr)[0])\n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#załadowanie danych szumu\n",
    "filenames = list(os.listdir('dane_wav/sound1_tiltLOW/Recorded'))\n",
    "data_sound_1 = np.vstack([parameters(2*((scipy.io.wavfile.read('dane_wav/sound1_tiltLOW/Recorded/'+x)[1]+32768)/65535)-1, scipy.io.wavfile.read('dane_wav/sound1_tiltLOW/Recorded/'+x)[0]) for x in filenames])\n",
    "\n",
    "#pogrupowanie danych według kierunku poziomego i stworzenie labeli(ograniczenie ilości kerunków, do 6), spłaszczenie danych\n",
    "LabelHorizontal = []\n",
    "LabelVertical = []\n",
    "\n",
    "with open(\"dane_wav/sound1_tiltLOW/motors_ground_truth\") as file: \n",
    "  for line in file:\n",
    "    line = line.strip().split('    ')\n",
    "    #print(line)\n",
    "    if float(line[1]) > -30 and float(line[1]) < 30 and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(1)\n",
    "    if float(line[1]) >= 30 and float(line[1]) <= 90 and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(2)\n",
    "    if float(line[1]) > 90 and float(line[1]) < 150 and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(3)\n",
    "    if (float(line[1]) >= 150 or float(line[1]) <= -150) and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(4)\n",
    "    if float(line[1]) > -150 and float(line[1]) < -90 and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(5)\n",
    "    if float(line[1]) >= -90 and float(line[1]) <= -30 and f'{line[0]}.wav' in filenames:\n",
    "      LabelVertical.append(6)\n",
    "#pogrupowanie danych według kierunku pionowego i stworzenie labeli(ograniczenie ilosci kierunków, do 3), spłaszcenie danych\n",
    "    if float(line[2]) > -60 and float(line[2]) < -21 and f'{line[0]}.wav' in filenames:\n",
    "      LabelHorizontal.append(1)\n",
    "    if float(line[2]) > -20 and float(line[2]) < 20 and f'{line[0]}.wav' in filenames:\n",
    "      LabelHorizontal.append(2)\n",
    "    if float(line[2]) > 21 and float(line[2]) < 60 and f'{line[0]}.wav' in filenames:\n",
    "      LabelHorizontal.append(3)\n",
    "#podział na zbiór uczący i testowy(dla dwóch wariantów-kierunek poziomy i pionowy)\n",
    "data_sound_1[np.isnan(data_sound_1)] = 0.0\n",
    "X_train_Ver, X_test_Ver, y_train_Ver, y_test_Ver = train_test_split(data_sound_1, LabelVertical, random_state=42)\n",
    "X_train_Hor, X_test_Hor, y_train_Hor, y_test_Hor = train_test_split(data_sound_1, LabelHorizontal, random_state=42)\n",
    "#normalizacja danych\n",
    "scaler = StandardScaler().fit(X_train_Ver)\n",
    "X_train_Ver = scaler.transform(X_train_Ver)\n",
    "X_test_Ver = scaler.transform(X_test_Ver)\n",
    "scaler = StandardScaler().fit(X_train_Hor)\n",
    "X_train_Hor = scaler.transform(X_train_Hor)\n",
    "X_test_Hor = scaler.transform(X_test_Hor)\n",
    "print(len(LabelHorizontal))\n",
    "print(len(LabelVertical))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLASYFIKATOR:\n",
    "klasyfikator 1->SVM\n",
    "klasyfikator 2->randomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 1, dane poziome\n",
    "SVM = SVC(C=1.0, random_state=42)\n",
    "SVM.fit(X_train_Ver, y_train_Ver)\n",
    "SVM_test_preds = SVM.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 1, dane pionowe\n",
    "SVM = SVC(C=1.0, random_state=42)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = SVM.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 2, dane poziome\n",
    "RandomForestClf = RandomForestClassifier(random_state=42)\n",
    "RandomForestClf.fit(X_train_Ver, y_train_Ver)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, RandomForestClf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 2, dane pionowe\n",
    "RandomForestClf = RandomForestClassifier(random_state=42)\n",
    "RandomForestClf.fit(X_train_Hor, y_train_Hor)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, RandomForestClf_test_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPtymalizacja hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcje klasyfikatora 1\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "model = SVC\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\"C\": trial.suggest_uniform(\"C\", 0, 1), \n",
    "           \"kernel\": trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'degree': trial.suggest_int('degree', 1,3)}\n",
    "    return space\n",
    "\n",
    "trials = 100 #liczba prob\n",
    "\n",
    "def objective(trial, model, get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 1 dla danych poziomych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model, get_space ,X_train_Ver, y_train_Ver), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 1 dla danych poziomych\n",
    "params = study.best_params\n",
    "SVM = SVC(random_state=42, **params)\n",
    "SVM.fit(X_train_Ver, y_train_Ver)\n",
    "SVM_test_preds = SVM.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 1 dla danych pionowych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model, get_space ,X_train_Hor, y_train_Hor), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 1 dla danych pionowych\n",
    "params = study.best_params\n",
    "SVM = SVC(random_state=42, **params)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = SVM.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcje klasyfikatora 2\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "model = RandomForestClassifier\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1)}\n",
    "    return space\n",
    "\n",
    "trials = 100 #liczba prób\n",
    "\n",
    "def objective(trial, model,get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 2 dla danych poziomych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model,get_space, X_train_Ver, y_train_Ver), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 2 dla danych poziomych\n",
    "params = study.best_params\n",
    "RandomForestClf = RandomForestClassifier(random_state=42, **params)\n",
    "RandomForestClf.fit(X_train_Ver, y_train_Ver)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, RandomForestClf_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 2 dla danych pionowych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model,get_space, X_train_Hor, y_train_Hor), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 2 dla danych pionowych\n",
    "params = study.best_params\n",
    "SVM = RandomForestClassifier(random_state=42)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = RandomForestClf.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PORÓWNANIE WYNIKÓW Z WYGENEROWANYMI DANYMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wygenerowanie białego szumu\n",
    "size = 44100\n",
    "WhiteNoise = np.random.normal(0, 1, size=size)\n",
    "\n",
    "#załadowanie hrtf'ów\n",
    "sofaDir = 'dane_hrtf/sofa/*.sofa'\n",
    "_SOFA = glob.glob(sofaDir)\n",
    "HRTFs = list([sofa.Database.open(_SOFA[x]) for x in range(5)])\n",
    "fs_H = HRTFs[0].Data.SamplingRate.get_values()[0]\n",
    "positions = HRTFs[0].Source.Position.get_values(system='spherical')\n",
    "#przefiltrowanie danych przez hrtf'y(stworzenie plików odpowiadających realnym danom)\n",
    "data = np.empty([5,len(positions),50], dtype=object)\n",
    "angles = np.empty([5,len(positions),2])\n",
    "for j in range(len(positions)):\n",
    "    for i in range(5):\n",
    "        angles[i,j] = [positions[j,0].round(),positions[j,1].round()]\n",
    "        H_L = HRTFs[i].Data.IR.get_values(indices={\"M\":j,\"R\":0, \"E\":0})\n",
    "        H_R = HRTFs[i].Data.IR.get_values(indices={\"M\":j,\"R\":1, \"E\":0})\n",
    "        L = (signal.fftconvolve(WhiteNoise, H_L))\n",
    "        R = (signal.fftconvolve(WhiteNoise, H_R))\n",
    "        mix = np.vstack([L,R])\n",
    "        data[i,j] = parameters(mix, int(fs_H))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, i=0, n=1):\n",
    "  \"\"\"\n",
    "  Combines dimensions of numpy array `a`, \n",
    "  starting at index `i`,\n",
    "  and combining `n` dimensions\n",
    "  \"\"\"\n",
    "  s = list(a.shape)\n",
    "  combined = functools.reduce(lambda x,y: x*y, s[i:i+n+1])\n",
    "  return np.reshape(a, s[:i] + [combined] + s[i+n+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pogrupowanie danych według kierunku poziomego i stworzenie labeli(ograniczenie ilości kerunków, do 6), spłaszczenie danych\n",
    "data1 = combine_dims(data,0,1)\n",
    "angles1 = combine_dims(angles, 0, 1)\n",
    "print(data1.shape)\n",
    "print(angles1.shape)\n",
    "LabelVertical = []\n",
    "LabelHorizontal = []\n",
    "for line in angles1:\n",
    "    if float(line[0]) > -30 and float(line[0]) < 30:\n",
    "      LabelVertical.append(1)\n",
    "    if float(line[0]) >= 30 and float(line[0]) <=90 :\n",
    "      LabelVertical.append(2)\n",
    "    if float(line[0]) > 90 and float(line[0]) < 150:\n",
    "      LabelVertical.append(3)\n",
    "    if float(line[0]) >= 150 or float(line[0]) <= -150:\n",
    "      LabelVertical.append(4)\n",
    "    if float(line[0]) > -150 and float(line[0]) < -90:\n",
    "      LabelVertical.append(5)\n",
    "    if float(line[0]) >= -90 and float(line[0]) <= -30:\n",
    "      LabelVertical.append(6)\n",
    "\n",
    "    if float(line[1]) < -20:  #pogrupowanie danych według kierunku pionowego i stworzenie labeli(ograniczenie ilosci kierunków, do 3), spłaszcenie danych\n",
    "      LabelHorizontal.append(1)\n",
    "    if float(line[1]) >= -20 and float(line[1]) <= 20:\n",
    "      LabelHorizontal.append(2)\n",
    "    if float(line[1]) > 20:\n",
    "      LabelHorizontal.append(3)\n",
    "#rint(len(LabelVertical))\n",
    "#print(len(LabelHorizontal))\n",
    "#podział na zbiór uczący i testowy(dla dwóch wariantów-kierunek poziomy i pionowy)\n",
    "X_train_Ver, X_test_Ver, y_train_Ver, y_test_Ver = train_test_split(data1, LabelVertical, random_state=42)\n",
    "X_train_Hor, X_test_Hor, y_train_Hor, y_test_Hor = train_test_split(data1, LabelHorizontal, random_state=42)\n",
    "#normalizacja danych\n",
    "scaler = StandardScaler().fit(X_train_Ver)\n",
    "X_train_Ver = scaler.transform(X_train_Ver)\n",
    "X_test_Ver = scaler.transform(X_test_Ver)\n",
    "scaler = StandardScaler().fit(X_train_Hor)\n",
    "X_train_Hor = scaler.transform(X_train_Hor)\n",
    "X_test_Hor = scaler.transform(X_test_Hor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLASYFIKATOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 1, dane poziome\n",
    "SVM = SVC(C=1.0, random_state=42)\n",
    "SVM.fit(X_train_Ver, y_train_Ver)\n",
    "SVM_test_preds = SVM.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 1, dane pionowe\n",
    "SVM = SVC(C=1.0, random_state=42)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = SVM.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 2, dane poziome\n",
    "RandomForestClf = RandomForestClassifier(random_state=42)\n",
    "RandomForestClf.fit(X_train_Ver, y_train_Ver)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, RandomForestClf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasyfikator 2, dane pionowe\n",
    "RandomForestClf = RandomForestClassifier(random_state=42)\n",
    "RandomForestClf.fit(X_train_Hor, y_train_Hor)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, RandomForestClf_test_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTYMALIZACJA HIPERPARAMETRÓW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcje klasyfikatora 1\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "model = SVC\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\"C\": trial.suggest_uniform(\"C\", 0, 1), \n",
    "           \"kernel\": trial.suggest_categorical(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'degree': trial.suggest_int('degree', 1,3)}\n",
    "    return space\n",
    "\n",
    "trials = 100 #liczba prob\n",
    "\n",
    "def objective(trial, model, get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 1 dla danych poziomych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model, get_space ,X_train_Ver, y_train_Ver), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 1 dla danych poziomych\n",
    "params = study.best_params\n",
    "SVM = SVC(random_state=42, **params)\n",
    "SVM.fit(X_train_Ver, y_train_Ver)\n",
    "SVM_test_preds = SVM.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 1 dla danych pionowych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model, get_space ,X_train_Hor, y_train_Hor), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 1 dla danych pionowych\n",
    "params = study.best_params\n",
    "SVM = SVC(random_state=42, **params)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = SVM.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcje klasyfikatora 2\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "model = RandomForestClassifier\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1)}\n",
    "    return space\n",
    "\n",
    "trials = 100 #liczba prób\n",
    "\n",
    "def objective(trial, model,get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 2 dla danych poziomych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model,get_space, X_train_Ver, y_train_Ver), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 2 dla danych poziomych\n",
    "params = study.best_params\n",
    "RandomForestClf = RandomForestClassifier(random_state=42, **params)\n",
    "RandomForestClf.fit(X_train_Ver, y_train_Ver)\n",
    "RandomForestClf_test_preds = RandomForestClf.predict(X_test_Ver)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Ver, RandomForestClf_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Ver, RandomForestClf_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Ver, RandomForestClf_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizacja klasyfikatora 2 dla danych pionowych\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model,get_space, X_train_Hor, y_train_Hor), n_trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obliczenie metryk sukcesu dla zoptymalizowanych hiperparamterów dla klasyfikatora 2 dla danych pionowych\n",
    "params = study.best_params\n",
    "SVM = RandomForestClassifier(random_state=42)\n",
    "SVM.fit(X_train_Hor, y_train_Hor)\n",
    "SVM_test_preds = RandomForestClf.predict(X_test_Hor)\n",
    "#metryki sukcesu\n",
    "print('test accuracy = ', accuracy_score(y_test_Hor, SVM_test_preds))\n",
    "print('test F1 = ', f1_score(y_test_Hor, SVM_test_preds, average='weighted'))\n",
    "print(confusion_matrix(y_test_Hor, SVM_test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
